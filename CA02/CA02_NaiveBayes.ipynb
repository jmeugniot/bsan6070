{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02-NaiveBayes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN32wkJbdGYsVy3QxxRFx8a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmeugniot/bsan6070/blob/main/CA02/CA02_NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgE1P-A3hDCo"
      },
      "source": [
        "#Naive Bayes\r\n",
        "###Email Spam Classification - CA02\r\n",
        "####Jamie Meugniot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ora7st2mePtP",
        "outputId": "09a3314d-65d0-4030-d1cf-907c8a88c63c"
      },
      "source": [
        "##import needed packages to complete Naive Bayes supervised machine learning algorithm\r\n",
        "import os #allows you to access a filesystem\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_Yw99Mth0kN"
      },
      "source": [
        "####Clean & Prep the data for the model\r\n",
        "#####Remove all non-required words (words that do not contribute to the analysis - aka Stop Words), espessions, and symbols from the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbGAkeW0hguz"
      },
      "source": [
        "#Create dictionary of 3000 most common words from all email content\r\n",
        "##Add all words and sympbols in the dictionary\r\n",
        "##Remove all non-alpha-numeric characters and any single character alpha-numeric characters\r\n",
        "##Shrink the dictionary keeping only 3000 most common words \r\n",
        "\r\n",
        "\r\n",
        "def create_dict (root_dir): #define new function as \"create_dict\"\r\n",
        "  total_words = [] #create empty list\r\n",
        "##construct an absolute path and join path components\r\n",
        "#\"f\" represents an absolute path, so all previous components of the path will be removed and the joining will continue\r\n",
        "#os.listdir will return a list containing the names of the entries in the directory given by the path\r\n",
        "  path = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\r\n",
        "  for email in path: #create for loop\r\n",
        "      with open(email) as m: #using 'with' means there is no need to call file.close()\r\n",
        "        for line in m:\r\n",
        "          words = line.split() #spliting strings into separate words\r\n",
        "          total_words += words ##add split words into the empty list total_words\r\n",
        "  word_dict = Counter(total_words) ##counter used to count the words saved in list total_words and saved in word_dict\r\n",
        "  words_to_remove = list(word_dict) #list() output returns a list of the interable's items in this case the words in word_dict\r\n",
        "\r\n",
        "\r\n",
        "  for item in words_to_remove: #create a for loop using the list from the previous step 'words_to_remove'\r\n",
        "  #create if statement - if the word in words_to_remove is not all alphabet letters then delete\r\n",
        "    if item.isalpha() == False: #isalpha will return a True or False if all characters of a value are alphabet letters\r\n",
        "      del word_dict[item] #delete the words that include non alphabet letters from word_dict\r\n",
        "  #use the len() to count the number of characters in a given word\r\n",
        "    elif len(item) == 1: #if the word is 1 character\r\n",
        "      del word_dict[item] #delete the words that equal 1 character from the word_dict\r\n",
        "  #words removed from word_dict\r\n",
        "  #.most_common will return ordered pairs of the word and its corresponding count in word_dict\r\n",
        "  word_dict = word_dict.most_common(3000) #set parameter of 3000 ordered pairs\r\n",
        "  return word_dict\r\n",
        "\r\n",
        " "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ulGu-HJdq8"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfdZ_Zu9tyN6"
      },
      "source": [
        "  def extract_features(mail_dir):\r\n",
        "  #os.listdir will return a list containing the names of the entries in the directory given by the path\r\n",
        "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)] #create list of files\r\n",
        "  ##create matrix of zeros using np.zeros. Set number of rows equal to the total files and the columns equal to the 3000 most commmon words\r\n",
        "    features_matrix = np.zeros((len(files),3000))\r\n",
        "  ##create 1 column of zeros equal to the total number of files\r\n",
        "    train_labels = np.zeros(len(files))\r\n",
        "    count = 1; #set count starting number\r\n",
        "    docID = 0; #set docID starting at 0\r\n",
        "    for emails in files: #iterate over each email file saved in the files path\r\n",
        "      with open(emails) as fi: #use \"fi\" to refer to the file object\r\n",
        "        for i, line in enumerate(fi): #enumerate counts the number of lines in the email\r\n",
        "          if i ==2: ##starting at the 3rd line which is were the email message starts \r\n",
        "            words = line.split() ##split the line of text into a list of words and store in list 'words'\r\n",
        "            for word in words: ##iterate over each word in the list of 'words'\r\n",
        "              wordID = 0 ##set wordID=0 for each word\r\n",
        "              for i, d in enumerate(word_dict):#enumerate is counting the number of words in the word_dict\r\n",
        "                if d[0] == word: #if d index 0 is equal to the word\r\n",
        "                  wordID = i #set wordID = i \r\n",
        "                  features_matrix[docID,wordID] = words.count(word)\r\n",
        "        train_labels[docID] = 0;\r\n",
        "        filepathTokens = emails.split('/') #split the email file name at each character '/'\r\n",
        "        lastToken = filepathTokens[len(filepathTokens)-1] #set lastToken as the the length of the file name after separation minus 1 - this gives us the final file name\r\n",
        "        if lastToken.startswith(\"spmsg\"): #if the file name starts with \"spmsg\"\r\n",
        "          train_labels[docID] = 1; #docID set to 1 to code spam files\r\n",
        "          count = count + 1 #increase count by 1 for each iteration\r\n",
        "        docID = docID + 1 #increase docID by 1 for each iteration\r\n",
        "    return features_matrix, train_labels  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhO9O7U6V6LC"
      },
      "source": [
        "####Main Program\r\n",
        "#####Call the above functions 'create_dict' and 'extract_features'. First it will train the model using the training dataset. After that it scores the test data set by running the trained model with the test data set. Final output will be the accuracy score of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXJZQjBVuDnN"
      },
      "source": [
        "##read in both data folders\r\n",
        "TRAIN_DIR = \"/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails\"\r\n",
        "TEST_DIR = \"/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HblROPqMfvB6",
        "outputId": "feaf9d36-1edb-4634-c2a2-a7c6b37deb3d"
      },
      "source": [
        "word_dict = create_dict(TRAIN_DIR) #use create_dict function and training data\r\n",
        "\r\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\") #print the text in quotes\r\n",
        "features_matrix, labels = extract_features(TRAIN_DIR) #create matrix using training data\r\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR) #create matrix using test data\r\n",
        "\r\n",
        "model = GaussianNB() #select the model to be used\r\n",
        "\r\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\r\n",
        "#train the model\r\n",
        "model.fit(features_matrix, labels) #fit function adjusts weights according to data values to that better accuracy can be achived\r\n",
        "print (\"Training completed\") #training is complete\r\n",
        "\r\n",
        "print (\"testing trained model to predict Test Data labels\")\r\n",
        "predicted_labels = model.predict(test_features_matrix) #predict function takes one argument and returns the learned label for each object\r\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\r\n",
        "print (accuracy_score(test_labels, predicted_labels)) ##accuracy score calculated based on test_labels matching predicted_labels"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}